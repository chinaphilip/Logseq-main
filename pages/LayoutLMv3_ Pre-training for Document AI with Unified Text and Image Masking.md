title:: LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking

- è¿™ç¯‡æ–‡ç« ä»–è¯´åˆ°æœ€åè¿™ä¸ªæ¨¡å‹åœ¨ä¸‹æ¸¸çš„ä»»åŠ¡ä¸­ä¸ä»…åœ¨ä»¥å›¾åƒä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸­è¡¨ç°çš„å¾ˆå¥½ï¼Œåœ¨ä»¥æ–‡å­—ä¸ºä¸ºä¸­å¿ƒçš„ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°çš„å¾ˆå¥½
- ![image.png](../assets/image_1665821891082_0.png){:height 502, :width 704}
- ![image.png](../assets/image_1665885850584_0.png)
-
- The Transformer has a multilayer architecture and each layer mainly consists of multi-head
  self-attention and position-wise fully connected feed-forward networks
- The input of Transformer is a concatenation of text embedding Y = y1:ğ¿ and image embedding X = x1:ğ‘€ sequences, where ğ¿ and ğ‘€ are sequence lengths for text and image respectively.
- We pre-processed document images with an off-the-shelf OCR toolkit to obtain textual content and corresponding 2D position information.
- æ–‡ç« è´¡çŒ®ï¼š
- 1ï¼‰ç¬¬ä¸€ä¸ªä¸ç”¨CNNçš„æ–‡æ¡£AIï¼ˆå½’åŠŸäºViLTï¼‰
- 2ï¼‰ä½¿ç”¨MIMå’ŒMLMï¼ŒWPAï¼ˆå½’åŠŸäºBeitï¼‰
- 3ï¼‰æ¨¡å‹çš„é€šç”¨æ€§å¾ˆå¥½ï¼Œåœ¨æ–‡æœ¬å’Œå›¾åƒä»»åŠ¡éƒ½å¾ˆå¥½ã€‚ï¼ˆTransformeré€‚ç”¨äºå¤šæ¨¡æ€ï¼‰
- 4ï¼‰å¤šä¸ªä»»åŠ¡ä¸Šçš„SOTA
- ViLT+Beit+Bert+layoutã€‚
-
  >Embedding
- **Text Embedding**
	- The position embeddings include 1D position and 2D layout position embeddings,
- **Layout Embedding**
	- The LayoutLM and LayoutLMv2 adopt word-level layout positions, where each word has its positions. Instead, we adopt segment-level layout positions that words in a segment share the same 2D position since the words usually express the same semantic meaning
- **Image Embedding**
	- ä¸åŒäºå…ˆå‰çš„æ–‡æ¡£ç†è§£å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¤§éƒ½éœ€è¦ä¸€ä¸ªCNNæˆ–è€…Faster-RCNNæŠ½å–å›¾ç‰‡ä¸Šçš„è§†è§‰ä¿¡æ¯ã€‚
	- ä»VITæ”¶åˆ°å¯å‘ï¼Œç›´æ¥é‡‡ç”¨VITçš„é¢„å¤„ç†ç­–ç•¥ï¼šå›¾ç‰‡resize(H\*W)->åˆ†patch->çº¿æ€§æ˜ å°„ï¼Œç»„æˆ$shape=[ h w / patch\_size^2, d ]$çš„tensorï¼Œå†åŠ ä¸Šå…¶1d position embeddingï¼Œ(2d layout informationè¢«æ‰”äº†ï¼Œå› ä¸ºä½œè€…å‘ç°ä»–æ²¡ç»™æ¨¡å‹æ•ˆæœå¸¦æ¥å¤šå¤§æå‡)å°±æ„æˆäº†è§†è§‰ç‰¹å¾ã€‚è¿™æ ·åšé™ä½äº†æ¨¡å‹å‚æ•°é‡ã€ç§»é™¤äº†ç›¸å¯¹è¾ƒå¤æ‚çš„é¢„å¤„ç†æµç¨‹ã€‚
	- LayoutLMv3 is the first multimodal model in Document AI that does not rely on CNNs to extract image features, which is vital to Document AI models to reduce parameters
	  or remove complex pre-processing steps
- **Self-attention**
  æ²¿ç”¨layoutLMv2çš„spatial aware self attentionã€‚
	- We insert semantic 1D relative position and spatial 2D relative position as bias terms in self-attention networks for text and image modalities following LayoutLMv2
-
  >Pre-training Objectives
- **Masked Language Modeling(MLM)**: æ¯æ¬¡maskæ‰30%çš„tokenã€‚
- ä¸å†æ˜¯éšæœºã€å­¤ç«‹çš„maskæ‰æŸä¸ªtokenï¼Œè€Œæ˜¯ä¸€æ¬¡æ€§maskä¸€å®šé•¿åº¦çš„tokensã€‚é•¿åº¦ä»æ³Šæ¾åˆ†å¸ƒï¼ˆÎ»=3ï¼‰ä¸­æŠ½æ ·ç¡®å®šã€‚
-
- **Masked Image Modeling(MIM)**: éšæœºmaskæ‰çº¦40%çš„image tokeï¼Œmaskç­–ç•¥é‡‡ç”¨blockwise masking strategyã€‚ï¼ˆå…·ä½“å‚ç…§[[Beit]]ï¼‰ã€‚
	- è¢«maskæ‰çš„patché€šè¿‡image tokenizerè½¬åŒ–ä¸ºç¦»æ•£çš„æ•°å­—ï¼ˆimage tokenizeræ¥æºäºé¢„è®­ç»ƒDiTï¼‰ï¼Œè¿›è€Œå¯¹è¿™äº›masked patchè¿›è¡Œé¢„æµ‹å³å¯ã€‚
	-
- **Word-Patch Alignment(WPA)**: å‰ä¿©é¢„è®­ç»ƒä»»åŠ¡æ²¡æœ‰æ˜¾å¼çš„åšvisualå’Œtextä¹‹é—´çš„æ¨¡æ€å¯¹é½ã€‚
- å¯¹äºå‰ä¸¤ä¸ªä»»åŠ¡æ‰€äº§ç”Ÿçš„è¾“å…¥ï¼Œè¿™é‡Œä¼šå¯¹æ¯ä¸ªtext tokenèµ‹äºˆ<aligned>ä»¥åŠ<unaligned>æ ‡ç­¾ã€‚
- å¯¹äºé‚£äº›text tokenæ²¡è¢«MLMæ‰ï¼Œä½†æ˜¯è¢«MIMæ‰ï¼ˆæ¯ä¸ªtext tokenä¼šæœ‰å…¶ä½ç½®ä¿¡æ¯çš„ï¼‰çš„ï¼Œèµ‹äºˆ<unaligned>æ ‡ç­¾ã€‚å¯¹äºé‚£äº›æ²¡è¢«MLMæ‰ï¼Œä¹Ÿæ²¡è¢«MINæ‰çš„ï¼Œèµ‹äºˆ<aligned>æ ‡ç­¾ã€‚
- å¯¹äºä»¥ä¸Šè¢«æ ‡è®°äº†çš„tokenï¼Œé€šè¿‡ä¸¤å±‚FCè¿›è¡Œé¢„æµ‹æ˜¯alignedè¿˜æ˜¯unalignedï¼Œæ‰€ä»¥æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»ã€‚
  è¿™é‡Œè¦å¿½ç•¥æ‰é‚£äº›è¢«MLMæ‰çš„text tokenï¼Œå³å®ƒä»¬ä¸å‚ä¸lossè®¡ç®—ï¼Œè¿™ä¹ˆåšäº‹ä¸ºäº†é˜²æ­¢æ¨¡å‹ä»masked textä»¥åŠimage patchä¹‹é—´å­¦åˆ°ä¸€äº›æ²¡ä»€ä¹ˆç”¨çš„å…³ç³»ã€‚
- åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­ï¼Œå¿…ç„¶ä¼šæœ‰ä¸€ä¸ªä»»åŠ¡æ˜¯å¯ä»¥å°†æ–‡æœ¬ä¸å›¾åƒäº§ç”Ÿè”ç³»çš„ã€‚å›é¡¾ v2 ç‰ˆæœ¬çš„æ¨¡å‹ï¼Œä¸»è¦é‡‡ç”¨çš„æ–¹æ³•æ˜¯å°†é€šè¿‡å°†ä¸åŒè¡Œçš„å›¾åƒæ–‡æœ¬è¿›è¡Œè¦†ç›–ï¼ˆå®é™…è¦†ç›–çš„èŒƒå›´æ˜¯ä¸€ä¸ª bboxï¼‰ ï¼Œç„¶åé€šè¿‡é¢„æµ‹æœªè¦†ç›–æ–‡æœ¬å¯¹åº”çš„å›¾åƒç‰‡æ®µæ˜¯å¦è¢«è¦†ç›–æ¥äº§ç”Ÿæ–‡æœ¬ä¸å›¾åƒçš„å…³è”æ€§ã€‚ä½†åœ¨ v3 ä¸­æ‰€æœ‰çš„å›¾åƒéƒ½æ˜¯é€šè¿‡ patch çš„æ–¹å¼ç›´æ¥æ˜ å°„ä¸ºå›¾åƒç‰¹å¾ï¼Œmask çš„æœ€å°å•ä½ä¸å†æ˜¯ bboxï¼Œè€Œæ˜¯ patch æœ¬èº«ã€‚é‚£ä¹ˆæŠŠ v2 çš„ä»»åŠ¡è¿›è¡Œç®€å•çš„è½¬æ¢ï¼Œv3 ç‰ˆæœ¬çš„ WPA å°±æ˜¯é¢„æµ‹è¢«è¦†ç›–æ–‡æœ¬å¯¹åº”çš„ patch æ˜¯å¦è¢«è¦†ç›–ã€‚
-
  >setting
- Transformer encoder with 12-head self-attention, hidden size of ğ· = 768, and 3,072 intermediate size of feed-forward networks.
- Our image tokenizer is initialized from a pre-trained image tokenizer in DiT, a self-supervised pre-trained document image Transformer model
- we tokenize the text sequence with Byte-Pair Encoding (BPE) [46] with a maximum sequence length ğ¿ = 512.
- The parameters for image embedding are ğ¶ Ã— ğ» Ã—ğ‘Š = 3 Ã— 224 Ã— 224, ğ‘ƒ = 16, ğ‘€ = 196.
- ç”¨åˆ°çš„èŠ‚çœæ˜¾å­˜å’ŒåŠ é€Ÿçš„æŠ€å·§
	- distributed and mixed-precision training
	- gradient accumulation mechanism to split the batch of samples into several mini-batches to overcome memory constraints for large batch sizes.
	- We further use a gradient checkpointing technique for document layout analysis to reduce memory costs
-
-
-
-
-
-
-
-
-
-
-
-
-