- https://zhuanlan.zhihu.com/p/407971299
- 这个专栏里面介绍的更全一些
- 总的来说把预训练语言模型与知识图谱的结合方式分为了两种，
-
  >一种是在embedding层面的结合，
- 隐式融合是比较直接的embedding融合方法，该类方法基于一些KGE（Knowledge Graph Embedding，使用最多的是TransE）算法获得知识图谱中的实体与关系的embedding，并为这些embedding修改预训练模型结构，以便将二者进行结合，这种方法方法需要重新训练语言模型
-
  >另一种是显式融合——不改变模型结构的融合方式                                                                            
  与之前所述的基于embedding的结合相比，另一种思路更为直接：既然知识图谱本身就是借助自然语言表达的，能不能直接把实体和关系经过某些变换后，以token的形式输入到预训练模型中？这样就不需要再去为每个实体学习embedding，更不需要考虑两种特征空间的聚合
-
- [[ERNIE]]
- [[ERINE-baidu]]
- [[K-bert]]
-
- [[Deep Short Text Classification with Knowledge Powered Attention]]
- [[Short Text Entity Linking with Fine-grained Topics]]这篇文章是上一篇文章中的一个参考文献，讲的是如何在短文本中涉及到的实体连接到知识图谱中的实体
-
-
-
-